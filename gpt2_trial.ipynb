{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quantitative-secondary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue 13 Jul 2021 07:51:17 PM CDT\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-knife",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2019/07/openai-gpt2-text-generator-python/\n",
    "https://medium.com/group-4-blog/extremely-simple-gpt-2-tutorial-3e22323aa384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "contrary-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONIOENCODING=UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "judicial-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "royal-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/doom/Documents/nn_work/gpt2_trials/gpt-2/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "considered-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import model, sample, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "anticipated-drain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_name = name of the model to use []\\nseed = determine the randomness\\nnsamples = number of samples to output\\nbatch_size = changes speed/memory.  Must divide nsamples,\\nlength = number of tokens in the generated text,\\ntemperature = controls the randomness, approaching zero it becomes deterministic,\\ntop_k = controls diversity of words to be considered - set to 1 only one word is considered for each step,\\nmodels_dir = directory where model lives\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    model_name = name of the model to use []\n",
    "    seed = determine the randomness\n",
    "    nsamples = number of samples to output\n",
    "    batch_size = changes speed/memory.  Must divide nsamples,\n",
    "    length = number of tokens in the generated text,\n",
    "    temperature = controls the randomness, approaching zero it becomes deterministic,\n",
    "    top_k = controls diversity of words to be considered - set to 1 only one word is considered for each step,\n",
    "    models_dir = directory where model lives\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rapid-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_model(\n",
    "    model_name,\n",
    "    seed,\n",
    "    nsamples,\n",
    "    batch_size,\n",
    "    length,\n",
    "    temperature,\n",
    "    top_k,\n",
    "    models_dir\n",
    "):\n",
    "    models_dir = f'/home/doom/Documents/nn_work/gpt2_trials/gpt-2/models/'\n",
    "    #models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    enc = encoder.get_encoder(model_name, models_dir)\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx // 2\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "        output = sample.sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            context=context,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k\n",
    "        )\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        while True:\n",
    "            raw_text = input(\"Model prompt >>> \")\n",
    "            while not raw_text:\n",
    "                print('Prompt should not be empty!')\n",
    "                raw_text = input(\"Model prompt >>> \")\n",
    "            context_tokens = enc.encode(raw_text)\n",
    "            generated = 0\n",
    "            for _ in range(nsamples // batch_size):\n",
    "                out = sess.run(output, feed_dict={\n",
    "                    context: [context_tokens for _ in range(batch_size)]\n",
    "                })[:, len(context_tokens):]\n",
    "                for i in range(batch_size):\n",
    "                    generated += 1\n",
    "                    text = enc.decode(out[i])\n",
    "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "                    print(text)\n",
    "            print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea734d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_options = ['1558M', '774M', '335M', '124M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-marina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/doom/Documents/nn_work/gpt2_trials/gpt-2/models/774M/model.ckpt\n",
      "Model prompt >>> 2+2\n",
      "======================================== SAMPLE 1 ========================================\n",
      "=9\n",
      "A=0.7. (3) If R^2 = 9 then\n",
      "Q=(A^1)+3+(3/6)(2)=1.2\n",
      "Q2/R=(2+3)=2/(3+R)/5 (R = 5, A 1 , R = 7 ) = 6/(4/7)+7\n",
      "This equation can be expressed graphatically using an arbitrary number as a \"unit cell\", as in \"1 cell\", but it\n",
      "======================================== SAMPLE 2 ========================================\n",
      "+0x80>\n",
      "It's possible the compiler can generate such function at linktime, or perhaps at build (which could happen, I have nothing concrete on that) or possibly it just takes a few days or hours to complete (and I doubt there'll have a link to \"compiles in time to 0.01\". It's possible it will compile and generate something else). If you know how to compile C with gcc then that might give an indication on how the C compiles to\n",
      "======================================== SAMPLE 3 ========================================\n",
      ". The \"1.0+\". \"2.0\" = 3+0 + 0, and 2*0 + 0 (i think that makes it clear why they have the +2). So it makes sense, that we should see that it makes some intuitive connection (if I remember correctly). I'll just give two example to explain. If i put in a \"2*(3 + 4)/(10^6+7)*10\", that should have 2 values: 10 +\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "interact_model(\n",
    "    model_name = model_options[1],\n",
    "    seed = None,\n",
    "    nsamples = 3, # number of samples to be generated\n",
    "    batch_size = 1,\n",
    "    length = 100,\n",
    "    temperature = 100,\n",
    "    top_k = 10,\n",
    "    models_dir = '/content/gpt-2/models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-stretch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2_work",
   "language": "python",
   "name": "gpt2_work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
